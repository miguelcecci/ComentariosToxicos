{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comments Classification\n",
    "\n",
    "Miguel A. Cece, Paulo Alves\n",
    "\n",
    "Neste arquivo estamos montando o vocabulario a partir da parte toxica do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/Toxic/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    replaced = re.sub('[^a-zA-Z0-9 | \\d | _]', ' ', text)\n",
    "    return replaced.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USANDO DATAFRAME REDUZIDO\n",
    "def create_lexicon(dataframe):\n",
    "    lexicon = []\n",
    "    for i in range(math.floor(len(dataframe))):\n",
    "        lexicon += word_tokenize(clean_text(dataframe.get_value(i, 'comment_text')))\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_words(lex, n_of_occurences, plot=False):\n",
    "    w_counts = Counter(lex)\n",
    "    w_counts_list = []\n",
    "    w_list = []\n",
    "    for w in w_counts:\n",
    "        if w_counts[w]>n_of_occurences:\n",
    "            w_list.append(w)\n",
    "            w_counts_list.append(w_counts[w])\n",
    "    zipada = sorted(zip(w_counts_list, w_list))\n",
    "    w_counts_list, w_list = zip(*zipada)\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "        y_pos = np.arange(len(w_list))\n",
    "        plt.bar(y_pos, w_counts_list, align='center')\n",
    "        plt.xticks(y_pos, w_list)\n",
    "        plt.ylabel('Numero de Ocorrencias')\n",
    "        plt.title('Palavas com mais de '+str(n_of_occurences)+' Ocorrencias')\n",
    "        plt.show()\n",
    "    return w_list, w_counts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \n",
      " Let me make this simple. I have had enough of your (and your AIDS fanatic friends) pathetic campaign against me. I am not playing this absurd game a moment longer. You and your friends at Dissident Action Group Exposed (MSN Group) can go take a flying leap. Please, please have me banned for this comment. I don't intended to do another thing for Wikipedia or to even use it in future. More time to work on the important campaign I have wasted time away from on this nonsense. You have done the dissident movement a great service in this regard. Nothing can change the truth of my life (truth is not the domain of Wikipedia) and the truth of yours. P. S. Reply if you wish but I will not ever be around here again to read your garbage. \n",
      "\n",
      "Clean Text \n",
      " let me make this simple  i have had enough of your  and your aids fanatic friends  pathetic campaign against me  i am not playing this absurd game a moment longer  you and your friends at dissident action group exposed  msn group  can go take a flying leap  please  please have me banned for this comment  i don t intended to do another thing for wikipedia or to even use it in future  more time to work on the important campaign i have wasted time away from on this nonsense  you have done the dissident movement a great service in this regard  nothing can change the truth of my life  truth is not the domain of wikipedia  and the truth of yours  p  s  reply if you wish but i will not ever be around here again to read your garbage \n"
     ]
    }
   ],
   "source": [
    "test = df.get_value(random.randint(0, len(df)), 'comment_text')\n",
    "print(\"Original\", \"\\n\",test,\"\\n\")\n",
    "test = clean_text(test)\n",
    "print(\"Clean Text\",\"\\n\",test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_lexicon(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "12      1             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_part = df[df.toxic + df.severe_toxic + df.obscene + df.threat + df.insult + df.identity_hate != 0]\n",
    "toxic_part.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16225 toxic coments on a 159571 coments dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"{} toxic coments on a {} coments dataset\".format(len(toxic_part), len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_lex = create_lexicon(toxic_part.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo o vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O a parte toxica do dataset tem um total de 35137 palavras unicas\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(toxic_lex))\n",
    "print(\"O a parte toxica do dataset tem um total de {} palavras unicas\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removendo palavras que aparecem menos de 3 vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, _ = repeated_words(toxic_lex, 2, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12292\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stmd_vocabulary = []\n",
    "for w in vocabulary:\n",
    "    if len(w) < 20:\n",
    "        stmd_vocabulary.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o stemmer o numero de palavras unicas é reduzido de 12292 para 12277\n"
     ]
    }
   ],
   "source": [
    "print(\"Usando o stemmer o numero de palavras unicas é reduzido de {} para {}\".format(len(vocabulary), len(stmd_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_array(text, vocabulary):\n",
    "    voc_array = np.zeros(len(vocabulary))\n",
    "    tok_text = word_tokenize(clean_text(text))\n",
    "    for j in tok_text:\n",
    "        count = 0\n",
    "        for i in vocabulary:\n",
    "            if j == i:\n",
    "                voc_array[count] += 1\n",
    "            count += 1\n",
    "    return voc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "_ = text_to_array(\"it is just the test test phrase\", vocabulary)\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_toxic_part =df[df.toxic + df.severe_toxic + df.obscene + df.threat + df.insult + df.identity_hate == 0]\n",
    "\n",
    "def load_batch(toxic, non_toxic, vocabulary, batch_size):\n",
    "    toxic = toxic.reset_index(drop=True)\n",
    "    non_toxic = non_toxic.reset_index()\n",
    "    toxic.head()\n",
    "    if batch_size % 2 != 0:\n",
    "        print(\"use a pair number to batch size\")\n",
    "        return -1\n",
    "    toxic = [text_to_array(toxic.get_value(random.randint(0,len(toxic)), 'comment_text'), vocabulary) for i in range(0,int(batch_size/2))]\n",
    "    non_toxic = [text_to_array(non_toxic.get_value(random.randint(0,len(non_toxic)), 'comment_text'), vocabulary) for i in range(0,int(batch_size/2))]\n",
    "    toxic = list(zip(toxic, [[0,1] for i in range(len(toxic))]))\n",
    "    non_toxic = list(zip(non_toxic, [[1,0] for i in range(len(non_toxic))]))\n",
    "    \n",
    "    batch = toxic + non_toxic\n",
    "    batch = random.sample(list(batch), batch_size)\n",
    "    return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1, test2 = load_batch(toxic_part, non_toxic_part, vocabulary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.,  0.,  0., ...,  0.,  0.,  1.]), array([ 0.,  0.,  0., ...,  1.,  0.,  3.]), array([ 0.,  0.,  0., ...,  1.,  0.,  0.]), array([  0.,   0.,   0., ...,  16.,   0.,   4.]), array([ 0.,  0.,  0., ...,  2.,  3.,  1.]), array([ 0.,  0.,  0., ...,  2.,  0.,  3.]), array([ 0.,  0.,  0., ...,  1.,  1.,  2.]), array([ 0.,  0.,  0., ...,  5.,  1.,  1.]), array([ 0.,  0.,  0., ...,  0.,  2.,  0.]), array([ 0.,  0.,  0., ...,  1.,  1.,  0.]))\n",
      "([0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias (shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_features = len(vocabulary)\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [batch_size, n_of_features])\n",
    "y_ = tf.placeholder(tf.float32, [batch_size, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_layer_one = tf.nn.relu(normal_full_layer(x, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = normal_full_layer(full_one_dropout, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  ON STEP:        0 - COST:         0.818954 - ACCURACY:         0.480000\n",
      ">  ON STEP:       10 - COST:         0.694072 - ACCURACY:         0.400000\n",
      ">  ON STEP:       20 - COST:         1.780575 - ACCURACY:         0.520000\n",
      ">  ON STEP:       30 - COST:         0.664452 - ACCURACY:         0.620000\n",
      ">  ON STEP:       40 - COST:         0.655377 - ACCURACY:         0.560000\n",
      ">  ON STEP:       50 - COST:         0.615464 - ACCURACY:         0.680000\n",
      ">  ON STEP:       60 - COST:         0.641629 - ACCURACY:         0.860000\n",
      ">  ON STEP:       70 - COST:         0.652393 - ACCURACY:         0.660000\n",
      ">  ON STEP:       80 - COST:         0.681532 - ACCURACY:         0.700000\n",
      ">  ON STEP:       90 - COST:         0.578380 - ACCURACY:         0.740000\n",
      ">  ON STEP:      100 - COST:         0.584049 - ACCURACY:         0.780000\n",
      ">  ON STEP:      110 - COST:         0.601543 - ACCURACY:         0.720000\n",
      ">  ON STEP:      120 - COST:         0.848181 - ACCURACY:         0.880000\n",
      ">  ON STEP:      130 - COST:         0.556856 - ACCURACY:         0.720000\n",
      ">  ON STEP:      140 - COST:         0.548451 - ACCURACY:         0.800000\n",
      ">  ON STEP:      150 - COST:         0.575623 - ACCURACY:         0.720000\n",
      ">  ON STEP:      160 - COST:         0.538572 - ACCURACY:         0.880000\n",
      ">  ON STEP:      170 - COST:         0.538015 - ACCURACY:         0.820000\n",
      ">  ON STEP:      180 - COST:         0.617733 - ACCURACY:         0.880000\n",
      ">  ON STEP:      190 - COST:         0.584250 - ACCURACY:         0.800000\n",
      ">  ON STEP:      200 - COST:         0.515941 - ACCURACY:         0.840000\n",
      ">  ON STEP:      210 - COST:         0.511217 - ACCURACY:         0.940000\n",
      ">  ON STEP:      220 - COST:         0.475466 - ACCURACY:         0.880000\n",
      ">  ON STEP:      230 - COST:         0.440063 - ACCURACY:         0.800000\n",
      ">  ON STEP:      240 - COST:         0.492904 - ACCURACY:         0.820000\n",
      ">  ON STEP:      250 - COST:         0.596696 - ACCURACY:         0.760000\n",
      ">  ON STEP:      260 - COST:         0.440864 - ACCURACY:         0.780000\n",
      ">  ON STEP:      270 - COST:         0.469718 - ACCURACY:         0.860000\n",
      ">  ON STEP:      280 - COST:         0.468372 - ACCURACY:         0.740000\n",
      ">  ON STEP:      290 - COST:         0.583781 - ACCURACY:         0.880000\n",
      ">  ON STEP:      300 - COST:         0.461679 - ACCURACY:         0.960000\n",
      ">  ON STEP:      310 - COST:         0.460294 - ACCURACY:         0.720000\n",
      ">  ON STEP:      320 - COST:         0.424088 - ACCURACY:         0.860000\n",
      ">  ON STEP:      330 - COST:         0.523193 - ACCURACY:         0.820000\n",
      ">  ON STEP:      340 - COST:         0.418225 - ACCURACY:         0.920000\n",
      ">  ON STEP:      350 - COST:         0.438681 - ACCURACY:         0.840000\n",
      ">  ON STEP:      360 - COST:         0.467509 - ACCURACY:         0.840000\n",
      ">  ON STEP:      370 - COST:         0.447831 - ACCURACY:         0.800000\n",
      ">  ON STEP:      380 - COST:         0.399874 - ACCURACY:         0.800000\n",
      ">  ON STEP:      390 - COST:         0.352462 - ACCURACY:         0.860000\n",
      ">  ON STEP:      400 - COST:         0.388369 - ACCURACY:         0.880000\n",
      ">  ON STEP:      410 - COST:         0.378638 - ACCURACY:         0.940000\n",
      ">  ON STEP:      420 - COST:         0.358170 - ACCURACY:         0.800000\n",
      ">  ON STEP:      430 - COST:         0.421531 - ACCURACY:         0.840000\n",
      ">  ON STEP:      440 - COST:         0.426306 - ACCURACY:         0.840000\n",
      ">  ON STEP:      450 - COST:         0.431582 - ACCURACY:         0.880000\n",
      ">  ON STEP:      460 - COST:         0.411391 - ACCURACY:         0.920000\n",
      ">  ON STEP:      470 - COST:         0.488652 - ACCURACY:         0.920000\n",
      ">  ON STEP:      480 - COST:         0.453200 - ACCURACY:         0.880000\n",
      ">  ON STEP:      490 - COST:         0.388330 - ACCURACY:         0.880000\n",
      "Model saved in path: ./seedlings-model.ckpt\n"
     ]
    }
   ],
   "source": [
    "plot_data = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(steps):\n",
    "        batch_x, batch_y = load_batch(toxic_part, non_toxic_part, vocabulary, batch_size)\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_y = np.array(batch_y)\n",
    "        feed = {x:batch_x, y_:batch_y, hold_prob:0.5}\n",
    "        sess.run(train, feed_dict = feed)\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            temp_plot = sess.run(cross_entropy, feed_dict = feed)\n",
    "            \n",
    "            matches = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_, 1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "#rodando acuracy fora do set de validacao\n",
    "            batch_x, batch_y = load_batch(toxic_part, non_toxic_part, vocabulary, batch_size)\n",
    "            accuracy = sess.run(acc, feed_dict={x:batch_x, y_:batch_y, hold_prob:1.0})\n",
    "#             accuracy = sess.run(acc, feed_dict={x:validation_data, y_:validation_label, hold_prob:1.0})\n",
    "#             plot_data.append([temp_plot, accuracy])\n",
    "#             if i%100 == 0:\n",
    "            print('>  ON STEP: {:8} - COST: {: 16.6f} - ACCURACY: {: >#16.6f}'.format(i, temp_plot, accuracy))\n",
    "#             print('>  ON STEP: {:8} - COST: {: 16.6f}'.format(i, temp_plot))\n",
    "            \n",
    "    #saving trained model\n",
    "    save_path = saver.save(sess, \"./seedlings-model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
